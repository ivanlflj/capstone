{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Analysis project\n",
    "## Data gathering\n",
    "The data was gathered from the Bovespa website in the historical quotes [link](http://www.bmfbovespa.com.br/pt_br/servicos/market-data/historico/mercado-a-vista/cotacoes-historicas/). The instructions to interpret the data are in the same website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# All imports\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "# Constants\n",
    "RANDOM_SEED = 42\n",
    "DAYS_AHEAD = 1 - 1  #DAYS_AHEAD = 0 is looking 1 day ahead, trying to know today with the data from the past\n",
    "GAIN_IN_TRAIN = 0.008 #The higher the number lower the number of trades but higher the certainty on it (in theory)\n",
    "\n",
    "# Data gathering\n",
    "stocks = ['ITSA4 ']#,'PETR4 ','VALE5 '] # Algorithm still need to remove 50 first days of stocks when using more than one.\n",
    "paths = ['stock_price\\\\inputs\\\\COTAHIST_A2006.TXT','stock_price\\\\inputs\\\\COTAHIST_A2007.TXT',\n",
    "        'stock_price\\\\inputs\\\\COTAHIST_A2008.TXT','stock_price\\\\inputs\\\\COTAHIST_A2009.TXT',\n",
    "        'stock_price\\\\inputs\\\\COTAHIST_A2010.TXT','stock_price\\\\inputs\\\\COTAHIST_A2011.TXT',\n",
    "        'stock_price\\\\inputs\\\\COTAHIST_A2012.TXT','stock_price\\\\inputs\\\\COTAHIST_A2013.TXT',\n",
    "        'stock_price\\\\inputs\\\\COTAHIST_A2014.TXT','stock_price\\\\inputs\\\\COTAHIST_A2015.TXT']\n",
    "df = pd.DataFrame()\n",
    "for stock in stocks:\n",
    "    for path in paths:\n",
    "        file = open(path,'r')\n",
    "        for line in file:\n",
    "            if (line[12:18] == stock):\n",
    "                df = df.append({'year': int(line[2:6]),'month': int(line[6:8]),'day': int(line[8:10]),\n",
    "                                'open': int(line[56:69])/100.,'high': int(line[69:82])/100.,\n",
    "                                'low': int(line[82:95])/100.,'close': int(line[108:121])/100.,\n",
    "                                'volume': int(line[152:170]), 'stock': stocks.index(stock)},ignore_index=True)\n",
    "        file.close\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical analysis\n",
    "The main data has already been gathered. Now it is time to use what is called technical analysis to improve the data we already have.\n",
    "\n",
    "The technical analysis consideres that the price movement of a stock reflects every thing that you need to know about the stock. The following image shows the analysis of a technical analist:\n",
    "\n",
    "![Analysis of a technical analist](http://www.liberatedstocktrader.com/wp-content/uploads/stock-market-analysis-sept-20-2010.jpg)\n",
    "\n",
    "To perform this analysis there is some indicators (calculated from the prizes) that are used for the behaviour prediction. We will use [Moving Averages](http://www.investopedia.com/terms/m/movingaverage.asp) for 5, 8, 21 and 50 periods.\n",
    "\n",
    "It is important to notice that we will only have the data for previous periods and not from our period. So to start we will make every date know about its 50 previous closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Knowing the past\n",
    "for i in tqdm(range(1,50 + 1)):\n",
    "    for moment in ('close','low','high','open'):\n",
    "        df[moment+str(i)] = -1\n",
    "        for j in range(50,len(df)):\n",
    "            df.loc[j,moment+str(i)] = df.loc[j-i,moment]\n",
    "\n",
    "# Verify if the code worked\n",
    "plt.scatter(df.index.tolist(),df['close'].values,c='c')\n",
    "plt.scatter(df.index.tolist(),df['close50'].values)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Moving averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in (5,8,21,50):\n",
    "    df['MA'+str(i)] = -1\n",
    "    for j in range(50,len(df)):\n",
    "        sum = 0\n",
    "        for k in range(1,i+1):\n",
    "            sum += df.loc[j,'close'+str(k)]\n",
    "        df.loc[j,'MA'+str(i)] = sum/i\n",
    "\n",
    "# Verify if the code worked\n",
    "plt.scatter(df.index.tolist(),df['close'].values,c='c')\n",
    "plt.plot(df.index.tolist(),df['MA50'].values,'-',c='k')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in (2,5,7,14):\n",
    "    df['RSI'+str(i)] = -1\n",
    "    df['AvgGain'+str(i)] = -1\n",
    "    df['AvgLoss'+str(i)] = -1\n",
    "    for j in range(50,51):\n",
    "        sum_gain = 0\n",
    "        sum_loss = 0\n",
    "        for k in range(1,i+1):\n",
    "            change = df.loc[j,'close'+str(k)] - df.loc[j,'close'+str(k+1)]\n",
    "            if change > 0:\n",
    "                sum_gain += change\n",
    "            else:\n",
    "                sum_loss -= change\n",
    "        df.loc[j,'AvgGain'+str(i)] = sum_gain/i\n",
    "        df.loc[j,'AvgLoss'+str(i)] = sum_loss/i\n",
    "        df.loc[j,'RSI'+str(i)] = (100 - 100/(1+(df.loc[j,'AvgGain'+str(i)]/df.loc[j,'AvgLoss'+str(i)])) if df.loc[j,'AvgLoss'+str(i)] != 0 else 0)\n",
    "    for j in range(51,len(df)):\n",
    "        sum_gain = 0\n",
    "        sum_loss = 0\n",
    "        change = df.loc[j,'close1'] - df.loc[j,'close2']\n",
    "        if change > 0:\n",
    "            sum_gain += change\n",
    "        else:\n",
    "            sum_loss -= change\n",
    "        df.loc[j,'AvgGain'+str(i)] = (df.loc[j-1,'AvgGain'+str(i)]*(i-1) + sum_gain)/i\n",
    "        df.loc[j,'AvgLoss'+str(i)] = (df.loc[j-1,'AvgLoss'+str(i)]*(i-1) + sum_loss)/i\n",
    "        df.loc[j,'RSI'+str(i)] = (100 - 100/(1+(df.loc[j,'AvgGain'+str(i)]/df.loc[j,'AvgLoss'+str(i)])) if df.loc[j,'AvgLoss'+str(i)] != 0 else 0)\n",
    "    df = df.drop(['AvgGain'+str(i),'AvgLoss'+str(i)],1)\n",
    "\n",
    "# Verify if the code worked\n",
    "plt.plot(df.index.tolist(),df['RSI14'].values,'-',c='k')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating day of the week\n",
    "\n",
    "We also want to know which day of the week it is. There is some studies that argue that a stock has a higher probability of moving up depending of the day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['week_day']=-1\n",
    "for i in range(len(df)):\n",
    "    df.loc[i,'week_day'] = datetime.datetime(int(df.loc[i,'year']), int(df.loc[i,'month']), int(df.loc[i,'day'])).weekday()\n",
    "# Just for information Monday to Sunday is 0 to 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume that we have are also from the last days. In the case of the volume we will gather only the volume for the last 5 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,5 + 1):\n",
    "    df['volume'+str(i)] = -1\n",
    "    for j in range(5,len(df)):\n",
    "        df.loc[j,'volume'+str(i)] = df.loc[j-i,'volume']\n",
    "\n",
    "# Verify if the code worked\n",
    "plt.scatter(df.index.tolist(),df['volume'].values,c='c')\n",
    "plt.scatter(df.index.tolist(),df['volume5'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the ups in the last 5 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    df['up'+str(i)] = df['close'+str(i)] - df['open'+str(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data to use\n",
    "Since the price of the stock varies alot, the important aspect here is the movement relative to the last price. So every price show be divided by the last price (close1) so we have the relative price of the movements until here.\n",
    "We will do the same for the volumes (volume1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[:,'close'] = df.loc[:,'close']/df.loc[:,'close1']\n",
    "df.loc[:,'low'] = df.loc[:,'low']/df.loc[:,'close1']\n",
    "df.loc[:,'high'] = df.loc[:,'high']/df.loc[:,'close1']\n",
    "df.loc[:,'open'] = df.loc[:,'open']/df.loc[:,'close1']\n",
    "\n",
    "for i in range(2,50+1):\n",
    "    df.loc[:,'close'+str(i)] = df.loc[:,'close'+str(i)]/df.loc[:,'close1']\n",
    "    df.loc[:,'low'+str(i)] = df.loc[:,'low'+str(i)]/df.loc[:,'close1']\n",
    "    df.loc[:,'high'+str(i)] = df.loc[:,'high'+str(i)]/df.loc[:,'close1']\n",
    "    df.loc[:,'open'+str(i)] = df.loc[:,'open'+str(i)]/df.loc[:,'close1']\n",
    "    \n",
    "for i in (5,8,21,50):\n",
    "    df.loc[:, 'MA'+str(i)] = df.loc[:, 'MA'+str(i)]/df.loc[:,'close1']\n",
    "\n",
    "for i in range(1,6):\n",
    "    df.loc[:,'up'+str(i)] = df.loc[:,'up'+str(i)]/df.loc[:,'close1']\n",
    "    \n",
    "df.loc[:,'close1'] = df.loc[:,'close1']/df.loc[:,'close1']\n",
    "df.loc[:,'low1'] = df.loc[:,'low1']/df.loc[:,'close1']\n",
    "df.loc[:,'high1'] = df.loc[:,'high1']/df.loc[:,'close1']\n",
    "df.loc[:,'open1'] = df.loc[:,'open1']/df.loc[:,'close1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same for the volumes (volume1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[:,'volume'] = df.loc[:,'volume']/df.loc[:,'volume1']\n",
    "\n",
    "for i in range(2,5+1):\n",
    "    df.loc[:,'volume'+str(i)] = df.loc[:,'volume'+str(i)]/df.loc[:,'volume1']\n",
    "    \n",
    "df.loc[:,'volume1'] = df.loc[:,'volume1']/df.loc[:,'volume1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "Since the data is now read, we need to erase the 50 first days. They do not have all the indicators that we need for the analysis and it can bring some problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.loc[50:].reset_index()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the target\n",
    "Until now we have not prepared the target, the target will be a data frame that gives 1 when price close higher than price open, otherwise it returns 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_all = []\n",
    "\n",
    "for i in range(len(df)-DAYS_AHEAD):\n",
    "    is_higher = (1 if (df.loc[i+DAYS_AHEAD,'close']/df.loc[i,'open'] > 1.) else 0)\n",
    "    y_all.extend([is_higher])\n",
    "    \n",
    "df.loc[:len(df)-DAYS_AHEAD-1,'up'] = y_all\n",
    "    \n",
    "df = df.dropna()\n",
    "\n",
    "plt.hist(y_all,bins=3)\n",
    "plt.show()\n",
    "\n",
    "len(y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is almost a 50% chance for the day to close higher or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future purpose we will create a y_all_narrow that returns days with a gain higher than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_all_narrow = []\n",
    "\n",
    "for i in range(len(df)-DAYS_AHEAD):\n",
    "    is_higher = (1 if (df.loc[i+DAYS_AHEAD,'close']/df.loc[i,'open'] > 1. + GAIN_IN_TRAIN) else 0)\n",
    "    y_all_narrow.extend([is_higher])\n",
    "    \n",
    "df.loc[:len(df)-DAYS_AHEAD-1,'up_narrow'] = y_all_narrow\n",
    "    \n",
    "df = df.dropna()\n",
    "\n",
    "plt.hist(y_all_narrow,bins=3)\n",
    "plt.show()\n",
    "\n",
    "len(y_all_narrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we go for the y_all_narrow we are aiming to much less data than if we aim to y_all, y_all_narrow is a little bit less than 1/3 of the total data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Now that we have the target we can clean our data to start the work. Removing index and data from the day, removing objective (up), removing day (since the systems can use it to overfit), removing constants (close1 and volume1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all = df.drop(['index','close','low','high','open','volume','up','up_narrow','day','close1','volume1'],1)\n",
    "\n",
    "for i in range(16,50+1):\n",
    "    X_all = X_all.drop(['close'+str(i),'low'+str(i),'high'+str(i),'open'+str(i)],1)\n",
    "\n",
    "X_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "Let's use the data to plot some small studies about the behaviour of the y_all.\n",
    "## Day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(321)\n",
    "plt.hist(df[df['week_day']==0]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Monday')\n",
    "plt.subplot(322)\n",
    "plt.hist(df[df['week_day']==1]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Tuesday')\n",
    "plt.subplot(323)\n",
    "plt.hist(df[df['week_day']==2]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Wednesday')\n",
    "plt.subplot(324)\n",
    "plt.hist(df[df['week_day']==3]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Thrusday')\n",
    "plt.subplot(325)\n",
    "plt.hist(df[df['week_day']==4]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Friday')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Month of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(431)\n",
    "plt.hist(df[df['month']==1]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Jan')\n",
    "plt.subplot(432)\n",
    "plt.hist(df[df['month']==2]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Feb')\n",
    "plt.subplot(433)\n",
    "plt.hist(df[df['month']==3]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Mar')\n",
    "plt.subplot(434)\n",
    "plt.hist(df[df['month']==4]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Apr')\n",
    "plt.subplot(435)\n",
    "plt.hist(df[df['month']==5]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('May')\n",
    "plt.subplot(436)\n",
    "plt.hist(df[df['month']==6]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Jun')\n",
    "plt.subplot(437)\n",
    "plt.hist(df[df['month']==7]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Jul')\n",
    "plt.subplot(438)\n",
    "plt.hist(df[df['month']==8]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Aug')\n",
    "plt.subplot(439)\n",
    "plt.hist(df[df['month']==9]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Sep')\n",
    "plt.subplot(4,3,10)\n",
    "plt.hist(df[df['month']==10]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Oct')\n",
    "plt.subplot(4,3,11)\n",
    "plt.hist(df[df['month']==11]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Nov')\n",
    "plt.subplot(4,3,12)\n",
    "plt.hist(df[df['month']==12]['up'],bins=2,rwidth=0.8)\n",
    "plt.title('Dec')\n",
    "plt.subplots_adjust(left=0.09, bottom=0.10, right=0.94, top=0.90, wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How last 5 days closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(231)\n",
    "plt.hist(df[df['up']==0]['up1'],range=(-0.1,0.1),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['up1'],range=(-0.1,0.1),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('up1')\n",
    "plt.subplot(232)\n",
    "plt.hist(df[df['up']==0]['up2'],range=(-0.1,0.1),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['up2'],range=(-0.1,0.1),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('up2')\n",
    "plt.subplot(233)\n",
    "plt.hist(df[df['up']==0]['up3'],range=(-0.1,0.1),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['up3'],range=(-0.1,0.1),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('up3')\n",
    "plt.legend()\n",
    "plt.subplot(234)\n",
    "plt.hist(df[df['up']==0]['up4'],range=(-0.1,0.1),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['up4'],range=(-0.1,0.1),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('up4')\n",
    "plt.subplot(235)\n",
    "plt.hist(df[df['up']==0]['up5'],range=(-0.1,0.1),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['up5'],range=(-0.1,0.1),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('up5')\n",
    "plt.subplots_adjust(left=0.09, bottom=0.10, right=0.94, top=0.90, wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How last volumes influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(231)\n",
    "plt.hist(df[df['up']==0]['volume1'],range=(0,10),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['volume1'],range=(0,10),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('volume1')\n",
    "plt.subplot(232)\n",
    "plt.hist(df[df['up']==0]['volume2'],range=(0,10),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['volume2'],range=(0,10),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('volume2')\n",
    "plt.subplot(233)\n",
    "plt.hist(df[df['up']==0]['volume3'],range=(0,10),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['volume3'],range=(0,10),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('volume3')\n",
    "plt.legend()\n",
    "plt.subplot(234)\n",
    "plt.hist(df[df['up']==0]['volume4'],range=(0,10),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['volume4'],range=(0,10),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('volume4')\n",
    "plt.subplot(235)\n",
    "plt.hist(df[df['up']==0]['volume5'],range=(0,10),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['volume5'],range=(0,10),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('volume5')\n",
    "plt.subplots_adjust(left=0.09, bottom=0.10, right=0.94, top=0.90, wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA's\n",
    "Some graphs about the Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['MA5'],range=(0.8,1.25),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['MA5'],range=(0.8,1.25),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('MA5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['MA8'],range=(0.7,1.3),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['MA8'],range=(0.7,1.3),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('MA8')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['MA21'],range=(0.8,1.5),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['MA21'],range=(0.8,1.5),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('MA21')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['MA50'],range=(0.8,1.8),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['MA50'],range=(0.8,1.8),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('MA50')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['RSI2'],range=(0,100),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['RSI2'],range=(0,100),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('RSI2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['RSI5'],range=(0,100),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['RSI5'],range=(0,100),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('RSI5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['RSI7'],range=(0,100),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['RSI7'],range=(0,100),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('RSI7')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df[df['up']==0]['RSI14'],range=(0,100),bins=10,label='Down')\n",
    "plt.hist(df[df['up']==1]['RSI14'],range=(0,100),bins=10,color='r',rwidth=0.5,label='Up')\n",
    "plt.title('RSI14')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "Here we are splitting the data between train and test. Since it does not look like a stratified data we are use a normal data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y_all, test_size = 0.25,\n",
    "                                                    stratify = y_all_narrow, random_state = RANDOM_SEED)\n",
    "X_train_narrow, X_test_narrow, y_train_narrow, y_test_narrow = train_test_split(X_all,y_all_narrow, test_size = 0.25,\n",
    "                                                    stratify = y_all_narrow, random_state = RANDOM_SEED)\n",
    "\n",
    "print(\"Is X_train equal:\" + str(X_train.equals(X_train_narrow)))\n",
    "print(\"Is X_test equal:\" + str(X_test.equals(X_test_narrow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "We will use the PCA to evaluate the features and the sklearn.feature_selection.SelectKBest to evaluate the features considering the target values. The objective is to reduce it for a more reasonable number. In the moment we have 213 features. We will  reduce the number to components to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "# ICA takes to long time to run, won't use it\n",
    "ica = FastICA(random_state = RANDOM_SEED)\n",
    "\n",
    "skb = SelectKBest(f_classif)\n",
    "parameters_skb = {'skb__k':[30, 50, 70]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: First I started with PCA and k=20 in SelectKBest, as mentioned, but the results were not good. Trying to improve I included FastICA and increased the number of variables in the SelectKBest. It improved the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the classifier\n",
    "Since we want to know if the stock will go up in a given day or not it is a classification problem. Considering the amount of data that we have, over 2000, we will use the Support Vector Classifier.\n",
    "\n",
    "For the classifier decision we are using the recommendation of the sklearn website, see image below:\n",
    "\n",
    "![sklearn map](http://scikit-learn.org/stable/_static/ml_map.png)\n",
    "\n",
    "Together with the classifier we will implement the GridSearchCV and the makescorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision_scorer = make_scorer(precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svc = SVC(random_state = RANDOM_SEED)\n",
    "\n",
    "parameters_svc = {'svc__kernel':('linear', 'poly', 'rbf', 'sigmoid'), 'svc__C':[1, 1.5, 3, 5, 10], 'svc__degree':[2,3,4]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SVC is not working lets go to KNeighborsClassifier, as the map suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knc = KNeighborsClassifier()\n",
    "\n",
    "parameters_knc = {'knc__n_neighbors':[2,4,5,8,16,32,64], 'knc__p':[2, 3], 'knc__leaf_size':[2,5,10,20,30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier also did not work well. Let's try Decision Tree and 2 ensemble methods, AdaBoostClassifier and RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state = RANDOM_SEED)\n",
    "\n",
    "parameters_dtc = {'dtc__max_depth':[5,8,15,25,30,None], 'dtc__min_samples_split':[1,2,5,10,15,100],\n",
    "              'dtc__min_samples_leaf':[1,2,5,10], 'dtc__max_features':('sqrt','log2',None)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(random_state = RANDOM_SEED)\n",
    "\n",
    "parameters_abc = {'abc__algorithm':('SAMME','SAMME.R'), 'abc__n_estimators':[10,30,50,80,120,300,500,800,1200],\n",
    "                  'abc__learning_rate':[0.1,0.5,1.0,1.5,2.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs = -1, random_state = RANDOM_SEED)\n",
    "\n",
    "#parameters_rfc = {'rfc__n_estimators':[10,30,50,80,120,300,500,800,1200], 'rfc__max_depth':[5,8,15,25,30,None],\n",
    "#              'rfc__min_samples_split':[1,2,5,10,15,100], 'rfc__min_samples_leaf':[1,2,5,10],\n",
    "#              'rfc__max_features':('sqrt','log2',None)}\n",
    "parameters_rfc = {'rfc__n_estimators':[50,120,300], 'rfc__max_depth':[5,30,None], 'rfc__max_features':('sqrt','log2',None),\n",
    "              'rfc__min_samples_split':[2,5,15], 'rfc__min_samples_leaf':[1,4,10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To run, using Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "possibilities = [['dtc',dtc,parameters_dtc],['knc',knc,parameters_knc],['abc',abc,parameters_abc],['rfc',rfc,parameters_rfc]]\n",
    "\n",
    "for possibility in tqdm(possibilities):\n",
    "    steps = [('skb',skb),(possibility[0], possibility[1])]\n",
    "    parameters = {}\n",
    "    parameters.update(parameters_skb)\n",
    "    parameters.update(possibility[2])\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=parameters, n_jobs = -1, scoring = precision_scorer, verbose = 1)\n",
    "    grid_search.fit(X_train, y_train_narrow)\n",
    "    pipeline = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Tuned %s model has a training precision score of %0.4f and test score of %0.4f.\" % \n",
    "          (possibility[0],precision_score(y_train, pipeline.predict(X_train)),precision_score(y_test, pipeline.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possibilities = [['dtc',dtc,parameters_dtc],['knc',knc,parameters_knc],['abc',abc,parameters_abc],['rfc',rfc,parameters_rfc]]\n",
    "\n",
    "for possibility in tqdm(possibilities):\n",
    "    steps = [('pca',pca),('skb',skb),(possibility[0], possibility[1])]\n",
    "    parameters = {}\n",
    "    parameters.update(parameters_skb)\n",
    "    parameters.update(possibility[2])\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=parameters, n_jobs = -1, scoring = precision_scorer, verbose = 1)\n",
    "    grid_search.fit(X_train, y_train_narrow)\n",
    "    pipeline = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Tuned %s model has a training precision score of %0.4f and test score of %0.4f.\" % \n",
    "          (possibility[0],precision_score(y_train, pipeline.predict(X_train)),precision_score(y_test, pipeline.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possibilities = [['dtc',dtc,parameters_dtc],['knc',knc,parameters_knc],['abc',abc,parameters_abc],['rfc',rfc,parameters_rfc]]\n",
    "\n",
    "for possibility in tqdm(possibilities):\n",
    "    steps = [('ica',ica),('skb',skb),(possibility[0], possibility[1])]\n",
    "    parameters = {}\n",
    "    parameters.update(parameters_skb)\n",
    "    parameters.update(possibility[2])\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=parameters, n_jobs = -1, scoring = precision_scorer, verbose = 1)\n",
    "    grid_search.fit(X_train, y_train_narrow)\n",
    "    pipeline = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Tuned %s model has a training precision score of %0.4f and test score of %0.4f.\" % \n",
    "          (possibility[0],precision_score(y_train, pipeline.predict(X_train)),precision_score(y_test, pipeline.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the model with the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possibilities = [['abc',abc,parameters_abc]]\n",
    "\n",
    "for possibility in tqdm(possibilities):\n",
    "    steps = [('pca',pca),('skb',skb),(possibility[0], possibility[1])]\n",
    "    parameters = {}\n",
    "    parameters.update(parameters_skb)\n",
    "    parameters.update(possibility[2])\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=parameters, n_jobs = -1, scoring = precision_scorer, verbose = 1)\n",
    "    grid_search.fit(X_train, y_train_narrow)\n",
    "    pipeline = grid_search.best_estimator_\n",
    "\n",
    "    print(\"Tuned %s model has a training precision score of %0.4f and test score of %0.4f.\" % \n",
    "          (possibility[0],precision_score(y_train, pipeline.predict(X_train)),precision_score(y_test, pipeline.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"From %d days it was considering to move up %d.\" %\n",
    "      (len(pipeline.predict(X_test)),np.count_nonzero(pipeline.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance of the traders\n",
    "The idea of the model is to serve as a signal for other trades. So it will give more chances for the intraday trades, for example. Anyway we will evaluate the performance of the method if we used it to buy and self stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes_buy = []\n",
    "\n",
    "for data in X_test:\n",
    "    if pipeline.predict(data):\n",
    "        indexes_buy.update(data.index.values)\n",
    "        \n",
    "performance = 0.\n",
    "\n",
    "for index in indexes_buy:\n",
    "    performance += df.loc[index, 'close'] - df.loc[index, 'open']\n",
    "    \n",
    "print(\"Considering every time the model sinalized the stock would go up, we bought one share we would have a profit of %0.2f\"\n",
    "     % performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
